{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a38bfb-1a3d-4ffc-afae-6317a1093d92",
   "metadata": {},
   "source": [
    "# 1. Check system environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "479e6252-e22f-49d2-a08e-715714d1abec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 09:57:05.152295: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-05 09:57:06.236580: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-05 09:57:06.236675: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-05 09:57:06.236684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b3bdec-a9ff-4682-b233-c79c8e58cb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor flow version : 2.11.0\n"
     ]
    }
   ],
   "source": [
    "# Check tensor flow version\n",
    "print(\"Tensor flow version : {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f5a2e4-7bef-4d49-b88a-23c24c20de5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 gpu available\n",
      "Device Name: /device:GPU:0   Device Type: GPU\n",
      "Device Name: /device:GPU:1   Device Type: GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 09:58:07.729014: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-05 09:58:09.408025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:04:00.0, compute capability: 7.5\n",
      "2023-01-05 09:58:09.409952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13582 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:82:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all logical GPU device on your notebook\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "gpu_nb = len(gpus)\n",
    "if gpus:\n",
    "    print(f\"We have {gpu_nb} gpu available\")\n",
    "for gpu in gpus:\n",
    "    print(\"Device Name:\", gpu.name, \"  Device Type:\", gpu.device_type)\n",
    "\n",
    "if gpu_nb == 0:\n",
    "    raise SystemError('No GPU device found') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb4a4cc-78e4-4006-9ed7-e9c76f88c58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1 cpu available\n",
      "Device Name: /device:CPU:0   Device Type: CPU\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all logical CPU device on your notebook\n",
    "cpus = tf.config.list_logical_devices('CPU')\n",
    "cpu_nb = len(cpus)\n",
    "if cpus:\n",
    "    print(f\"We have {cpu_nb} cpu available\")\n",
    "for cpu in cpus:\n",
    "    print(\"Device Name:\", cpu.name, \"  Device Type:\", cpu.device_type)\n",
    "\n",
    "if cpu_nb == 0:\n",
    "    raise SystemError('No CPU device found') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e513b8-25e3-4c3b-91ba-a47057f0b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list for available GPU devices names\n",
    "gpu_names = [x.name for x in gpus]\n",
    "# Build a list for available CPU devices names\n",
    "cpu_names = [x.name for x in cpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5970564e-8662-469c-adf5-ff17284f124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n",
      "/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "print(gpu_names[0])\n",
    "print(cpu_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deecedc-003d-4423-ab23-94349db58d67",
   "metadata": {},
   "source": [
    "# 2. Define the operation to benchmark\n",
    "\n",
    "To avoid downloading data, here we choose to define a simple function that multiply 2 random vectors of the given length. This is the function that we are going to benchmark over available devices (GPU and CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d4c144-cafd-4a08-b0db-9920cdc6d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_multiply(vector_length):\n",
    "    vector_1 = tf.random.normal(vector_length)\n",
    "    vector_2 = tf.random.normal(vector_length)\n",
    "    return vector_1 * vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f56c9c-b441-422a-9400-f79ef4854722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_operation(vector_length):\n",
    "    # If you have several GPU you can select the one to use by changing the used index\n",
    "    with tf.device(gpu_names[0]):\n",
    "        random_multiply(vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f0a9292-edb7-48cd-ae6d-b90c85ddbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_operation(vector_length):\n",
    "    # If you have several CPU you can select the one to use by changing the used index\n",
    "    with tf.device(cpu_names[0]):\n",
    "        random_multiply(vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deace98-ea05-4db4-830f-c88338403085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Launch the benchmark of each device over several vectors of different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf86a2af-ec83-4a44-994b-d84c95f4b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations on vector of length 1 are 0.629850086262482x faster on GPU than CPU\n",
      "Operations on vector of length 10 are 0.6582134305981338x faster on GPU than CPU\n",
      "Operations on vector of length 100 are 0.6801479576564236x faster on GPU than CPU\n",
      "Operations on vector of length 1000 are 0.7547438530466873x faster on GPU than CPU\n",
      "Operations on vector of length 10000 are 1.5366686471375353x faster on GPU than CPU\n",
      "Operations on vector of length 100000 are 1.9995076976573303x faster on GPU than CPU\n",
      "Operations on vector of length 1000000 are 5.044044423726084x faster on GPU than CPU\n",
      "Operations on vector of length 10000000 are 59.587983243386155x faster on GPU than CPU\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu_operation([1])\n",
    "gpu_operation([1])\n",
    "\n",
    "for i in range(8):\n",
    "    vector_length = pow(10, i)\n",
    "    cpu_time = timeit.timeit(f'cpu_operation([{vector_length}])', number=20, setup=\"from __main__ import cpu_operation\")\n",
    "    gpu_time = timeit.timeit(f'gpu_operation([{vector_length}])', number=20, setup=\"from __main__ import gpu_operation\")\n",
    "    print(f'Operations on vector of length {vector_length} are {cpu_time/gpu_time}x faster on GPU than CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d9545-d618-4195-b37f-61070c5bcbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

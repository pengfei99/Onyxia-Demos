{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e547c21d-1bfc-4fe4-9653-3db9dda2a5c1",
   "metadata": {},
   "source": [
    "# Spark on GPU\n",
    "\n",
    "Datalab also provide gpu for data analysis, this notebook shows you how to use GPU in a spark session. You can also use GPU for model training (e.g. tensorflow, pytorch, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c9563a-7328-40f6-bf0a-fc886a1eeff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession \n",
    "         .builder\n",
    "         .master(\"k8s://https://kubernetes.default.svc:443\")\n",
    "         .config(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n",
    "         .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n",
    "         .config(\"spark.executor.instances\", \"5\")\n",
    "         .config(\"spark.executor.memory\", \"4g\")\n",
    "         .config(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n",
    "         \n",
    "         # GPU specifique configuration\n",
    "         .config(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "         .config(\"spark.task.resource.gpu.amount\", \"1\")\n",
    "         .config(\"spark.executor.resource.gpu.discoveryScript\", \"/opt/spark/examples/src/main/scripts/getGpusResources.sh\")\n",
    "         .config(\"spark.executor.resource.gpu.vendor\", \"nvidia.com\")\n",
    "         # spark.rapids.sql.enabled permet d'utiliser les gpus aussi pour les etapes SQL d'ETL. La pertinance de faire ça est à étudier => voir la vidéo mentionné en intro\n",
    "         .config(\"spark.rapids.sql.enabled\", \"true\")\n",
    "         .config(\"spark.rapids.sql.incompatibleOps.enabled\", \"true\")\n",
    "         .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "         .config(\"spark.rapids.force.caller.classloader\", \"false\")\n",
    "\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f50447-c1f2-4132-8c9a-fd0a2dec465a",
   "metadata": {},
   "source": [
    "## Check worker number\n",
    "As the number of GPUs is limited inside the cluster, so you may not get the worker number that you have asked. Below are two ways to check the worker number\n",
    "\n",
    "### Check the worker number via Spark UI\n",
    "\n",
    "To check how many workers with gpu has been deoployed. You can use the spark UI to view the status of all the workers.\n",
    "\n",
    "### Check the worker number via kubectl\n",
    "You can also use below command to check your spark worker number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52597aa0-4e63-4266-972f-8acc24420fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                    READY   STATUS    RESTARTS   AGE\n",
      "pyspark-shell-f5aecc83465f6d1b-exec-1   1/1     Running   0          20m\n"
     ]
    }
   ],
   "source": [
    "! kubectl get pods -l spark-role=executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27db8b9-64cb-46b0-ad29-f24003ca3f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:         pyspark-shell-f5aecc83465f6d1b-exec-1\n",
      "Namespace:    user-pengfei\n",
      "Priority:     0\n",
      "Node:         boss11/192.168.253.171\n",
      "Start Time:   Fri, 16 Sep 2022 12:56:43 +0000\n",
      "Labels:       spark-app-selector=spark-application-1663333003497\n",
      "              spark-exec-id=1\n",
      "              spark-exec-resourceprofile-id=0\n",
      "              spark-role=executor\n",
      "Annotations:  kubernetes.io/psp: default\n",
      "Status:       Running\n",
      "IP:           10.233.127.232\n",
      "IPs:\n",
      "  IP:           10.233.127.232\n",
      "Controlled By:  Pod/rapidsai-264860-0\n",
      "Containers:\n",
      "  spark-kubernetes-executor:\n",
      "    Container ID:  containerd://320701628668923c51e18af36cd192db9ccef2ba8f8ad95eb15dd66b48d54be6\n",
      "    Image:         inseefrlab/rapidsai:cuda11.0-spark3.2.0\n",
      "    Image ID:      docker.io/inseefrlab/rapidsai@sha256:71e9f007a5bb0e775b07e2a04bd5615457e582cd897a7f118e1bc3c1ab526aed\n",
      "    Port:          7079/TCP\n",
      "    Host Port:     0/TCP\n",
      "    Args:\n",
      "      executor\n",
      "    State:          Running\n",
      "      Started:      Fri, 16 Sep 2022 12:56:50 +0000\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Limits:\n",
      "      memory:          4505Mi\n",
      "      nvidia.com/gpu:  1\n",
      "    Requests:\n",
      "      cpu:             1\n",
      "      memory:          4505Mi\n",
      "      nvidia.com/gpu:  1\n",
      "    Environment:\n",
      "      SPARK_USER:                 root\n",
      "      SPARK_DRIVER_URL:           spark://CoarseGrainedScheduler@rapidsai-264860-0.rapidsai-264860.user-pengfei.svc.cluster.local:34427\n",
      "      SPARK_EXECUTOR_CORES:       1\n",
      "      SPARK_EXECUTOR_MEMORY:      4096m\n",
      "      SPARK_APPLICATION_ID:       spark-application-1663333003497\n",
      "      SPARK_CONF_DIR:             /opt/spark/conf\n",
      "      SPARK_EXECUTOR_ID:          1\n",
      "      SPARK_RESOURCE_PROFILE_ID:  0\n",
      "      SPARK_EXECUTOR_POD_IP:       (v1:status.podIP)\n",
      "      SPARK_JAVA_OPT_0:           -Dspark.driver.port=34427\n",
      "      SPARK_LOCAL_DIRS:           /var/data/spark-eb51e44a-669e-4ee1-84b7-0e80583f4e67\n",
      "    Mounts:\n",
      "      /opt/spark/conf from spark-conf-volume-exec (rw)\n",
      "      /var/data/spark-eb51e44a-669e-4ee1-84b7-0e80583f4e67 from spark-local-dir-1 (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vfhn8 (ro)\n",
      "Conditions:\n",
      "  Type              Status\n",
      "  Initialized       True \n",
      "  Ready             True \n",
      "  ContainersReady   True \n",
      "  PodScheduled      True \n",
      "Volumes:\n",
      "  spark-conf-volume-exec:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      spark-exec-8d852483465f71d2-conf-map\n",
      "    Optional:  false\n",
      "  spark-local-dir-1:\n",
      "    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n",
      "    Medium:     \n",
      "    SizeLimit:  <unset>\n",
      "  kube-api-access-vfhn8:\n",
      "    Type:                    Projected (a volume that contains injected data from multiple sources)\n",
      "    TokenExpirationSeconds:  3607\n",
      "    ConfigMapName:           kube-root-ca.crt\n",
      "    ConfigMapOptional:       <nil>\n",
      "    DownwardAPI:             true\n",
      "QoS Class:                   Burstable\n",
      "Node-Selectors:              <none>\n",
      "Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n",
      "                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\n",
      "Events:\n",
      "  Type    Reason     Age   From               Message\n",
      "  ----    ------     ----  ----               -------\n",
      "  Normal  Scheduled  22m   default-scheduler  Successfully assigned user-pengfei/pyspark-shell-f5aecc83465f6d1b-exec-1 to boss11\n",
      "  Normal  Pulling    22m   kubelet            Pulling image \"inseefrlab/rapidsai:cuda11.0-spark3.2.0\"\n",
      "  Normal  Pulled     22m   kubelet            Successfully pulled image \"inseefrlab/rapidsai:cuda11.0-spark3.2.0\" in 2.424563224s\n",
      "  Normal  Created    22m   kubelet            Created container spark-kubernetes-executor\n",
      "  Normal  Started    22m   kubelet            Started container spark-kubernetes-executor\n"
     ]
    }
   ],
   "source": [
    "! kubectl describe pods pyspark-shell-f5aecc83465f6d1b-exec-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4fe398-01ea-461f-9e7a-e1085791f9fb",
   "metadata": {},
   "source": [
    "## Do some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "969eea74-cf7a-48d0-837c-caf7a74c8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir=\"s3a://pengfei\"\n",
    "parquet_file_name=\"diffusion/data_format/sf_fire/parquet/raw\"\n",
    "data_path=f\"{work_dir}/{parquet_file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a55ed2ec-e866-4d75-ae67-99e98cb2cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw=spark.read.parquet(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ed6a72e-9aac-45fd-918c-23ba2834ed14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame has : 5500519 rows and 34 columns\n"
     ]
    }
   ],
   "source": [
    "row_nb=df_raw.count()\n",
    "col_nb=len(df_raw.columns)\n",
    "\n",
    "print(f\"data frame has : {row_nb} rows and {col_nb} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd92b4ec-6356-4e44-9aba-6ee16b838fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fed31bc-44c4-4c39-98bf-708e1bc9c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=df_raw.sample(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e181424d-08f3-4f2f-b19a-a5f47a66e9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sample 0.0, 0.1, false, 7648596361908242375\n",
      "+- GpuColumnarToRow false\n",
      "   +- GpuFileGpuScan parquet [CallNumber#151,UnitID#152,IncidentNumber#153,CallType#154,CallDate#155,WatchDate#156,ReceivedDtTm#157,EntryDtTm#158,DispatchDtTm#159,ResponseDtTm#160,OnSceneDtTm#161,TransportDtTm#162,HospitalDtTm#163,CallFinalDisposition#164,AvailableDtTm#165,Address#166,City#167,ZipcodeofIncident#168,Battalion#169,StationArea#170,Box#171,OriginalPriority#172,Priority#173,FinalPriority#174,... 10 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[s3a://pengfei/diffusion/data_format/sf_fire/parquet/raw], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<CallNumber:int,UnitID:string,IncidentNumber:int,CallType:string,CallDate:string,WatchDate:...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d4cb9a1-6866-40a6-b01d-21067506b963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|            CallType|incident_number|\n",
      "+--------------------+---------------+\n",
      "|    Medical Incident|        3596332|\n",
      "|      Structure Fire|         681179|\n",
      "|              Alarms|         599263|\n",
      "|   Traffic Collision|         224909|\n",
      "|               Other|          87468|\n",
      "|Citizen Assist / ...|          82173|\n",
      "|        Outside Fire|          68491|\n",
      "|        Water Rescue|          28253|\n",
      "|        Vehicle Fire|          25512|\n",
      "|Gas Leak (Natural...|          22961|\n",
      "+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top10CallType=df_raw.groupBy(\"CallType\").agg(count(\"IncidentNumber\").alias(\"incident_number\")).orderBy(col(\"incident_number\").desc())\n",
    "top10CallType.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a11e0-faa6-4f12-9a07-7e7441d40b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
